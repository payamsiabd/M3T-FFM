# Hierarchical Multi-Modal Multi-Task Federated Foundation Model (HF-FM)

This repository contains the official implementation for **Hierarchical Multi-Modal Multi-Task Federated Foundation Model (HF-FM)**, a system that integrates vision-language foundation models with hierarchical federated learning (FL) to enable efficient, scalable, and personalized training across heterogeneous edge networks.

## ğŸ” Overview

HF-FM is designed to support diverse **modalities** (e.g., vision, language), multiple **tasks** (e.g., classification, grounding, captioning), and federated **hierarchies** (users â†’ edge â†’ cloud). It uses modular adapters and task heads to support dynamic task execution and personalization across users, edge nodes, and cloud servers.

Key features:

- Multi-modal support via pretrained ViLT backbone
- Multi-task heads (classification, grounding, etc.)
- Adapter-based personalization with low memory usage
- Hierarchical FL orchestration with device-to-device (D2D) aggregation
- Energy- and latency-aware simulation engine

## ğŸŒ Network Model

The HF-FM system's network model is inspired by and extends the following works:

- (https://ieeexplore.ieee.org/abstract/document/9705093): Multi-Stage Hybrid Federated Learning Over Large-Scale D2D-Enabled Fog Networks
- (https://ieeexplore.ieee.org/document/10304380): Delay-Aware Hierarchical Federated Learning
- (https://arxiv.org/abs/2404.06324): Dynamic D2D-Assisted Federated Learning over O-RAN: Performance Analysis, MAC Scheduler, and Asymmetric User Selection
- (https://ieeexplore.ieee.org/document/9148862): Client-Edge-Cloud Hierarchical Federated Learning

## ğŸ“ Project Structure

```
HFFM/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ datasets.py              # Dataset loading and preprocessing
â”‚   â”œâ”€â”€ models.py                # Adapter-based model definitions
â”‚   â”œâ”€â”€ network.py               # Communication & aggregation logic
â”‚   â””â”€â”€ utils.py                 # Helper functions
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ dataset_generator_*.py   # Scripts to generate balanced datasets
â”‚   â”œâ”€â”€ *_vocab_balanced.py      # Balanced vocab files for tasks (ART, GQA, VizWiz)
â”‚
â”œâ”€â”€ methods/
â”‚   â”œâ”€â”€ main_hierarchy.py        # Entry point for hierarchical FL
â”‚   â”œâ”€â”€ main_local.py            # Entry point for local-only training
â”‚   â”œâ”€â”€ table_generator.py       # Summarize evaluation metrics
â”‚   â””â”€â”€ results/                 # Folder to store results and logs
```

## ğŸ“¦ Installation

```bash
git clone https://github.com/payamsiabd/M3T-FFM.git

pip install -r requirements.txt
```

## ğŸš€ Run Training

```bash
python train.py --config configs/hf_fm.yaml
```

You can also change adapter mode, task type, or aggregation settings via the configuration file.

## ğŸ“Š Results

All accuracy/loss/energy/latency traces are stored in `results/`. Plots and comparisons are automatically generated for:

- Accuracy vs. Epoch
- Loss vs. Energy/Latency
- System-wide performance under different aggregation schemes

## ğŸ“„ License

This project is released under the MIT License.

## ğŸ‘¤ Author

**Payam Abdisarabshali** â€“ Ph.D. Student, Electrical Engineering, SUNY Buffalo

For questions or collaborations, feel free to contact via GitHub or [Google Scholar](https://scholar.google.com/citations?user=ksQpR00AAAAJ&hl=en).

---

Enjoy exploring the future of federated foundation models! ğŸš€
